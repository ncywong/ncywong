---
layout: post
title: Polynomial approximations, sensitivity analysis and compressive sensing
mathjax: true
---

To introduce this blog, I thought I would provide some exposition to the background of my *actual* work in computational engineering that I have been conducting. This should serve as a *tl;dr* of my masters thesis and a (hopefully) quick and succinct answer to the question of what I do. 

## Computational Engineering and Models

Imagine that we have an *engineering model*, which is basically a black box which accepts inputs and spits out an output. Models should be meaningful. For instance, we can develop a model to simulate the physics of a machine, or represent the relationship between the symptoms and diagnosis of a patient. A model can be defined by equations, where we obtain an output by chugging through equations with certain values, but often we have a data-driven model, where all we know about the model is some *simulations*, where someone tells us the outputs corresponding to some inputs they decided.

The task of a computational engineer is to wrangle with these models. We want to be able to tell something useful from them, with as little effort as possible. For example, what is the most efficient design of the machine? If I have a patient with symptoms A, B and C, what is her diagnosis?

However, models are complicated. They could take days to evaluate. It could also take days to even figure out what the model really is, if the data is difficult to obtain. This is where an *approximation* would come in handy.

## Polynomial approximations

We want a approximation that is:

* Simple to evaluate
* Scalable to high dimensions
* Conducive to easy statistical calculations

To this end we choose [orthogonal polynomials](https://en.wikipedia.org/wiki/Orthogonal_polynomials):

* Easy to see why it is simple evaluate! In addition, gradients are easy to evaluate as well, making it useful for gradient based optimization.
* We multiply polynomials in different dimensions to construct multivariate polynomials. There is some trickery behind this with regards to *which* polynomials we choose.
* If we treat the input as random (in reality, we always have uncertainty and variations in our designs), calculating stats of the output amounts to calculating integrals, and orthogonal polynomials are excellent for this purpose using quadrature. One basic method is [Gauss quadrature](https://en.wikipedia.org/wiki/Gaussian_quadrature).

The form of a P-term polynomial approximation is:

$$
f(s) = \sum_{i=1}^{P} x_i \phi_i (s),
$$

where each $$\phi_i(s)$$ is a polynomial (for simplicity, we can think of assigning each $$\phi_i$$ to one input variable $$s_i$$). and we need to find a suitable set of $$x_i$$.

## Compressive sensing (CS)

How is this related to compressive sensing? Well, first let's have a brief overview of what CS is. In full, CS is a rather non-intuitive technique which came from signal processing, but in our case I find it most convenient to view it as an optimization problem. Let's say we have want to solve for a vector $$\mathbf{x}$$ in a problem like

$$
\mathbf{Ax = b},
$$

where $$A$$ is a *fat* matrix, i.e. more columns than rows. Linear algebra tells us that there are potentially infinite solutions to this problem. We want to select the *best* solution, in some sense. In computational engineering we always prefer a simpler model over a complicated one, so we want a simple, or *minimal* solution. How do we define this mathematically? We may compare solutions by its 2-norm (i.e. Euclidean length). But, in fact what we want here is a minimal *0-norm* solution. This is a solution with the fewest non-zero entries. 

Why? First we should mention why we are interested in this matrix problem at all. It turns out that each row of the equation corresponds to one simulation of our real model. For example, we performed an experiment using some inputs $$\mathbf{s}$$ and got an output of $$b$$. We want then want our model to satisfy $$\sum \phi_i(s) x_i = b$$, which corresponds to one row of the matrix problem if we plug the values of $$\phi_i(s)$$ into $$\mathbf{A}$$ (a good exercise in linear algebra...). By solving $$\mathbf{x}$$ we get the $$x_i$$'s in the model.

 Imagine we have a lot of zero $$x_i$$ 's. Wouldn't that imply that a lot of the corresponding terms in the sum can effectively be ignored? In other words, by finding the minimal 0-norm solution, the **sparsest** solution, we find the simplest model, i.e. the best model! This is the essence of CS. My work involved coding up algorithms to efficiently (using as few rows of $$A$$ as possible) do this.

## Sensitivity analysis

Now that we have a good approximation model, what can we do with it? Recall that we think of variations in the input as random, and our inputs are random variables. Now, a sensible way of quantifying *the importance of a variable* would be the amount of variation it causes in the output. In other words, we measure the output variance with some random inputs, chop it up and assign each portion to each input variable and see which one gets the largest portion. This is called sensitivity analysis.

It turns out that using orthogonal polynomials makes this super easy. As mentioned above, orthogonal polynomials simplify integrals, which is exactly what we need to do here. In fact, the importance of a variable under this formulation is proportional to the square of the corresponding $$x_i$$ to the appropriate term in the approximation model ([Sobol' indices](https://en.wikipedia.org/wiki/Variance-based_sensitivity_analysis)). Note how remarkably painless this is than computing an integral (which in general need to be approximated with thousands of experiments...)

My work involved experimenting with new measures of sensitivity that involves skewness (third central moment) instead of variance. Since the third central moment is signed (can be negative) as opposed to the varianced based indices being always positive, this may reveal something better... More in a paper coming soon.

Hopefully this all is a bit useful and coherent, as I have always found explaining what I do a bit of a challenge :)